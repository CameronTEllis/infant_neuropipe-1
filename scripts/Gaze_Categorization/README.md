# Offline tools for coding gaze

## Setting up 

In this directory, you will find the scripts and file structure to perform offline coding of gaze. This takes in the timing file and video that are returned from the framegrabber script, collected using the [experiment_menu](https://github.com/ntblab/experiment_menu). This then runs a psychtoolbox script to allow coders to go through participants looking data frame by frame and code each individually. In theory, this same code could be used to manually code any video data, although the set up herein is advised to minimize headaches with changing this code. Moreover, this code could be adjusted not just to code gaze but also other video information you might care about.

It is expected that you would set this code up for a testing computer in your lab space, not the computer you will do other infant_neuropipe analyses on. This is because this code needs to be run on a computer with Psychtoolbox 3 (the PTB functionality is minimal and so is likely flexible to most versions) and so wouldn't be appropriate for cluster computing. Make sure to be secure about how you store the frames. 

## Adding a participant's data

The [framegrabber](https://github.com/ntblab/experiment_menu/blob/master/Scripts/Utils_EyeTracker_FrameGrabber_Record.py) code generates a participant directory that is generated on the eye tracking computer, which includes an avi video file and a file called 'TimingFile.txt'. The former is all the frames that were recorded, the latter contains the timing information for each frame, as well as each message that is received via the ethernet connection. To create a new participant for gaze coding, add a folder to the the '$PROJ_DIR/scripts/Gaze_Categorization/Frames' directory in this repo with the participant's name (same as their matlab file name). Then put the avi file, 'TimingFile.txt' and matlab file (containing data and experiment timing information) in that folder. 

## Setting up a participant

To run the script, you need to run '$PROJ_DIR/scripts/Gaze_Categorization/scripts/Gaze_Categorization.m' in the PTB compatible matlab. You will need to supply two inputs: the coder name and the participant's matlab name. The coder name should be just 2 initials followed by an underscore and a number (e.g. 'CE_1'). The two initials identify the coder and each coder should be unique. This allows for coder reliability comparisons. The number refers to which interpolation 'condition' the coder is in. The code defaults to show coders every frame if they are in condition 1. Ideally you would have at least two coders, giving you measures of intraframe reliability (how often do participants that saw the same frame, report the same response) and interframe reliability (do participants who code adjacent frames report the same response). 

If you want to change the interpolation then go to the 'Gaze_Categorization' script and change the 'Interpolation' variable under the section near the top labelled 'PARAMETERS YOU MIGHT WANT TO CHANGE'. If you set it to 5 then participants will see every 5th frame. You can change other parameters too: 'Rate' is the forced delay between frames in order to control the rate at which participants can code, 'ScalingFactor' is the size upsampling of the frames which can be increased to help the visibility for the coder, 'TrialsperCalibrationDisplay' is the frequency with which participants will be reshown the calibration period for training (described more below). 'VignetteSize' is the vertical height of the vignette that coders will see. This means you can control how much of the image a coder sees to facilitate privacy. Finally, 'im_smoothing' is the pixel size of the median filter kernel, helping to clean up any white noise in the image. 

The first time you run this code, it will take 1-10 minutes to collect all of the timing and frame information from the avi file (via the $PROJ_DIR/scripts/Gaze_Categorization/scripts/generate_image_list.m script). Specifically, this will convert each frame of the movie into an image and identify which frames correspond to which experiment, block, repetition and trial. You will see the experiments that were found in the timing file printed in the console window. If you don't see your experiment appear, there is likely an issue with your experiment being recognized. Read the section below about adding an experiment. You will also get information about the timing. Critically, there will be lag in your set up between when your code thinks a stimulus onsets and when it appears in the video. Look at the lag section below for details on how to detect and fix this.

Once this is done, it will save this image information to the participant folder in 'Frames'. If at anytime you change the experiments that ought to be coded, you will need to delete the 'ImageList.mat' file in order to code these frames. **WARNING**, the code will struggle if different coders were run on different versions of ImageList. It is not recommended to change this file once you have collected coder data.

After the frame images are generated and checked, a PTB screen will appear. For participant data that includes [eye tracker calibration](https://github.com/ntblab/experiment_menu/blob/master/Scripts/Experiment_EyeTrackerCalib.m), the coder will be shown a series of trials where stimuli are flashed in specific locations and so saccades to that location are expected. Coders will be shown a looping video of each trial as well as information about the expected direction of the saccade; hence giving a reference of a given saccade type. At any point, the coder can skip this by pressing 's'. To help keep the coders calibrated, these phases are presented at the start of the session and dispersed throughout according to the 'TrialsperCalibrationDisplay' variable.

## Coding a participant

Once a participant's data has been set up the coding blocks will begin.  Coders are given the options for the possible responses for the following trials and must use these keys to categorize each frame. If they want to go back, they can press 'b'. Different experiments will allow for different types of response coding. For instance, for some experiments it only matters whether their eyes are open and so you might only code 'present' vs 'undetected'. For other experiments it might matter whether they look 'left', 'center' or 'right'. Within each block of coding type the possible trials are all randomized in order to increase coder naivety. 

Sometimes, frame specific information will be displayed in the bottom left. This can be useful if you have periods of time where coding is either really important (you can encourage focus) or where different response options are unlikely (and you don't mind biasing coding for these epochs). To add or edit this, look at the function `scripts/image_specific_instructions.m`.

Coders are always able to exit the gaze coding script by pressing ‘q’ and choosing to resume coding at a later time. Once gaze coding is complete, the output will be a mat file in the 'Coder_Files' directory of the eye tracking computer. To further analyze this participant with infant_neuropipe, you can put this coder file in a subject-specific directory ($SUBJ_DIR/data/Behavior/$SUBJ_Coder_$CODER.mat), or have a scriot to this automatically by putting the data in the 'Coder_Files' directory of your cluster-based computer ($PROJ_DIR/scripts/Gaze_Categorization/Coder_Files/) and running the '$PROJ_DIR/scripts/Gaze_Categorization/scripts/run_transfer_coder_files.sh'

It is possible to enable functionality to have the coders trace the eye. If 'draw_window' is set 1 then this means that on a single frame for every trial the coder will be asked to draw with the mouse a box around the eye. This is turned off by default because it is time consuming and likely irrelevant; however, if you want to use the functionality it is available.

## Training coders

[Here](./training_coders.md) is a document we have prepared to train coders who are naive to the set up. Read over this to learn some more nuances of the code and adjust it to suit your needs.

There is a script to help you replay the coded responses overtop of the frames they were based on called `Gaze_Categorization_Replay.m`. This is helpful for reviewing any potential disagreements between coders.

## Adding new experiments
To add a new experiment, you'll need to read/edit `$PROJ_DIR/scripts/Gaze_Categorization_Responses.m`. If you look at the section at the top of this script it says there is one variable that will need to change to add a new coder('ExperimentDefinitions'), and two others that are optional to add suiting your needs ('ResponseStr' and 'ResponseAllowed').

Moreover, look at the last section of the script entitled '#### IF YOU WANT TO ADD AN EXPERIMENT, THEN APPEND IT HERE ####' and add your experiment to the end of the 'ExperimentDefinitions' list. You will need to list the title of your experiment as it is written in the Experiment_Menu system. Then, add in the relevant timing variables outputted by your Experiment_Menu experiment specific script. As a default, you can use Start_of_Block and End_of_Block variables if all of the time the block is running is relevant for gaze coding. However, if for example, some parts of the block are relevant and others are not (i.e. the start and ends of trials), you'll need to specify the variables that will give timing information as to when in the experiment the frames that are relevant for your analyses were being recorded. 

## Adding new response options
Within the 'Gaze_Categorization_Responses' there is code that details which keypresses denote different possible gaze coding directions (i.e. a look to the left is coded as the 'a' key) as well as keypresses that will trigger commands within the Gaze Coding (i.e. to go back a frame you press the 'b' key). To add a new keypress, add a variable name and keypress under the comment titled: 'What are the different eye fixation key presses'. You will then need to assign a unique number to your new key press variable as well--you will see that there are comments following those variable assignments that say %Otherwise coded as #%. Once you have done that, you may add in the String Name you would like to call your keypress into the 'ResponseNames' variable, and then will need to append the end of 'key_code_mapping' with variable you just assigned above. The key_code_mapping variable is beneath the section entitled ' #### IF YOU WANT TO ADD A KEY, THEN APPEND IT HERE ####'

Directly below that there is a section entitled '#### IF YOU WANT TO ADD AN ALLOWED RESPONSE CATEGORY, THEN APPEND IT HERE ####'. Here is where you can create a new Response_Allowed_code variable specific to your experiment. What these do is allow you to specify which keypresses are relevant for your experiment. For example, if you added a new keypress above that is used to denote a Look_Top (as in when a particpant's eyes are looking towards a stimulus that might appear at the top of the screen), and you wanted coders in your experiment to delineate between looks to the top, looks center and looks offscreen, you would reference these numbers (the numbers being defined by which position your keypress variable holds in the key_code_mapping list) in your ResponseAllowed_code variable. When making a new ResponseAllowed_code variable, you should add the next subsequent number following the ones already listed as the struct index (e.g., ResponseAllowed_code{3}=[13,3,0]).

Finally, take a look down at the section that says: #### IF YOU WANT TO ADD AN EXPERIMENT, THEN APPEND IT HERE #### where you've already listed your experiment name and the timing variables used to determine which frames are relevant to your gaze coding. You should append the number referenced above to the end of this line so the code will know which key presses are relevant for your responses. After editing this, the code should now know the mapping of keypresses to eye fixation categories, which combination of these responses are relevant for your experiment, and the timing for frames in your experiment that need to be grabbed for the gaze categorization. 

## Detecting and Fixing lag
With any recording equipment, there is lag between when information hits the recording instrument and when it is stored. In our set up the lag is about 120ms. This means if we put the camera at the projector screen showing a stop watch, the time at which the frame is recorded on the computer will be approximately 120ms ahead of the time recorded on the display. We discovered this magnitude of lag by doing this and other similar tests (e.g. when a flash onsets at a specific time according to the matlab code, what time does that correspond to in eye tracker time). We spent a lot of time with our system checking that the lag was consistent: under some setup regimes the lag is variable which is disastrous. We strongly advise that you do extensive lag testing before collecting participant data in order to avoid a headache later. Once you know the lag, you can edit the lag by changing the `$PROJ_DIR/scripts/Gaze_Categorization/scripts/generate_image_list.m` script to have a specific lag. 

Every time this script runs, it gives you the chance to check that the lag is appropriately dealt with for a given participant. In particular it will tell you the time stamp, in eye tracker time when a block is thought to onset. If a block onset produces a big luminance change then you can often see that in the reflection of a participant's eyes (when using a high-resolution camera especially). You can search for the frame with the time stamp closest to the one supplied and determine whether there was a luminance change at that time. If so, the lag has been fixed. To check whether the lag is consistent over the duration of the experiment, this cript produces a scatter plot of the difference in expected and actual eye tracker times across the experiment. This is based on the handshakes between the eye tracker computer and experiment menu computer. There should be minimal variability, ideally the range will be less than 10ms.



